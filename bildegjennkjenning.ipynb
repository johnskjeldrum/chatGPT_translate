{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'Invalid content type. image_url is only supported by certain models.', 'type': 'invalid_request_error', 'param': 'messages.[0].content.[1].type', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\xo103\\Menon Economics AS\\14795 Oversettelse av næringsrapporter - Dokumenter\\General\\Data og analyse\\bildegjennkjenning.ipynb Cell 1\u001b[0m line \u001b[0;36m<cell line: 83>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xo103/Menon%20Economics%20AS/14795%20Oversettelse%20av%20n%C3%A6ringsrapporter%20-%20Dokumenter/General/Data%20og%20analyse/bildegjennkjenning.ipynb#W0sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xo103/Menon%20Economics%20AS/14795%20Oversettelse%20av%20n%C3%A6ringsrapporter%20-%20Dokumenter/General/Data%20og%20analyse/bildegjennkjenning.ipynb#W0sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m                 \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMax retries reached. Exiting.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/xo103/Menon%20Economics%20AS/14795%20Oversettelse%20av%20n%C3%A6ringsrapporter%20-%20Dokumenter/General/Data%20og%20analyse/bildegjennkjenning.ipynb#W0sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m test \u001b[39m=\u001b[39m api_call_gpt_with_retry(\u001b[39m\"\u001b[39;49m\u001b[39mRead the data from the picture below\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\xo103\\Menon Economics AS\\14795 Oversettelse av næringsrapporter - Dokumenter\\General\\Data og analyse\\bildegjennkjenning.ipynb Cell 1\u001b[0m line \u001b[0;36mapi_call_gpt_with_retry\u001b[1;34m(message_content, max_retries, delay)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xo103/Menon%20Economics%20AS/14795%20Oversettelse%20av%20n%C3%A6ringsrapporter%20-%20Dokumenter/General/Data%20og%20analyse/bildegjennkjenning.ipynb#W0sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m base64_string \u001b[39m=\u001b[39m image_to_base64(\u001b[39m\"\u001b[39m\u001b[39mimage1.png\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xo103/Menon%20Economics%20AS/14795%20Oversettelse%20av%20n%C3%A6ringsrapporter%20-%20Dokumenter/General/Data%20og%20analyse/bildegjennkjenning.ipynb#W0sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mfor\u001b[39;00m attempt \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_retries): \n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/xo103/Menon%20Economics%20AS/14795%20Oversettelse%20av%20n%C3%A6ringsrapporter%20-%20Dokumenter/General/Data%20og%20analyse/bildegjennkjenning.ipynb#W0sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     response \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xo103/Menon%20Economics%20AS/14795%20Oversettelse%20av%20n%C3%A6ringsrapporter%20-%20Dokumenter/General/Data%20og%20analyse/bildegjennkjenning.ipynb#W0sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m                 model\u001b[39m=\u001b[39;49m os\u001b[39m.\u001b[39;49mgetenv(\u001b[39m\"\u001b[39;49m\u001b[39mMODEL_NAME\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xo103/Menon%20Economics%20AS/14795%20Oversettelse%20av%20n%C3%A6ringsrapporter%20-%20Dokumenter/General/Data%20og%20analyse/bildegjennkjenning.ipynb#W0sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m                 messages\u001b[39m=\u001b[39;49m[\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xo103/Menon%20Economics%20AS/14795%20Oversettelse%20av%20n%C3%A6ringsrapporter%20-%20Dokumenter/General/Data%20og%20analyse/bildegjennkjenning.ipynb#W0sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m                     {\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xo103/Menon%20Economics%20AS/14795%20Oversettelse%20av%20n%C3%A6ringsrapporter%20-%20Dokumenter/General/Data%20og%20analyse/bildegjennkjenning.ipynb#W0sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m                         \u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xo103/Menon%20Economics%20AS/14795%20Oversettelse%20av%20n%C3%A6ringsrapporter%20-%20Dokumenter/General/Data%20og%20analyse/bildegjennkjenning.ipynb#W0sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m                         \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: [\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xo103/Menon%20Economics%20AS/14795%20Oversettelse%20av%20n%C3%A6ringsrapporter%20-%20Dokumenter/General/Data%20og%20analyse/bildegjennkjenning.ipynb#W0sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m                             {\u001b[39m\"\u001b[39;49m\u001b[39mtype\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m: message_content},\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xo103/Menon%20Economics%20AS/14795%20Oversettelse%20av%20n%C3%A6ringsrapporter%20-%20Dokumenter/General/Data%20og%20analyse/bildegjennkjenning.ipynb#W0sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m                             {\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xo103/Menon%20Economics%20AS/14795%20Oversettelse%20av%20n%C3%A6ringsrapporter%20-%20Dokumenter/General/Data%20og%20analyse/bildegjennkjenning.ipynb#W0sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m                                 \u001b[39m\"\u001b[39;49m\u001b[39mtype\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mimage_url\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xo103/Menon%20Economics%20AS/14795%20Oversettelse%20av%20n%C3%A6ringsrapporter%20-%20Dokumenter/General/Data%20og%20analyse/bildegjennkjenning.ipynb#W0sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m                                 \u001b[39m\"\u001b[39;49m\u001b[39mimage_url\u001b[39;49m\u001b[39m\"\u001b[39;49m: {\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xo103/Menon%20Economics%20AS/14795%20Oversettelse%20av%20n%C3%A6ringsrapporter%20-%20Dokumenter/General/Data%20og%20analyse/bildegjennkjenning.ipynb#W0sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m                                     \u001b[39m\"\u001b[39;49m\u001b[39murl\u001b[39;49m\u001b[39m\"\u001b[39;49m: base64_string,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xo103/Menon%20Economics%20AS/14795%20Oversettelse%20av%20n%C3%A6ringsrapporter%20-%20Dokumenter/General/Data%20og%20analyse/bildegjennkjenning.ipynb#W0sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m                                     \u001b[39m\"\u001b[39;49m\u001b[39mdetail\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mlow\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xo103/Menon%20Economics%20AS/14795%20Oversettelse%20av%20n%C3%A6ringsrapporter%20-%20Dokumenter/General/Data%20og%20analyse/bildegjennkjenning.ipynb#W0sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m                                 }\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xo103/Menon%20Economics%20AS/14795%20Oversettelse%20av%20n%C3%A6ringsrapporter%20-%20Dokumenter/General/Data%20og%20analyse/bildegjennkjenning.ipynb#W0sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m                             },\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xo103/Menon%20Economics%20AS/14795%20Oversettelse%20av%20n%C3%A6ringsrapporter%20-%20Dokumenter/General/Data%20og%20analyse/bildegjennkjenning.ipynb#W0sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m                         ],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xo103/Menon%20Economics%20AS/14795%20Oversettelse%20av%20n%C3%A6ringsrapporter%20-%20Dokumenter/General/Data%20og%20analyse/bildegjennkjenning.ipynb#W0sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m                     }\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xo103/Menon%20Economics%20AS/14795%20Oversettelse%20av%20n%C3%A6ringsrapporter%20-%20Dokumenter/General/Data%20og%20analyse/bildegjennkjenning.ipynb#W0sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m                 ],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xo103/Menon%20Economics%20AS/14795%20Oversettelse%20av%20n%C3%A6ringsrapporter%20-%20Dokumenter/General/Data%20og%20analyse/bildegjennkjenning.ipynb#W0sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m                 max_tokens\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xo103/Menon%20Economics%20AS/14795%20Oversettelse%20av%20n%C3%A6ringsrapporter%20-%20Dokumenter/General/Data%20og%20analyse/bildegjennkjenning.ipynb#W0sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m             )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xo103/Menon%20Economics%20AS/14795%20Oversettelse%20av%20n%C3%A6ringsrapporter%20-%20Dokumenter/General/Data%20og%20analyse/bildegjennkjenning.ipynb#W0sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xo103/Menon%20Economics%20AS/14795%20Oversettelse%20av%20n%C3%A6ringsrapporter%20-%20Dokumenter/General/Data%20og%20analyse/bildegjennkjenning.ipynb#W0sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent\n",
      "File \u001b[1;32mc:\\Users\\xo103\\.conda\\envs\\geo_env\\lib\\site-packages\\openai\\_utils\\_utils.py:299\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    297\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    298\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 299\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\xo103\\.conda\\envs\\geo_env\\lib\\site-packages\\openai\\resources\\chat\\completions.py:594\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    549\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    550\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    592\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    593\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 594\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[0;32m    595\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    596\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[0;32m    597\u001b[0m             {\n\u001b[0;32m    598\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[0;32m    599\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[0;32m    600\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[0;32m    601\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[0;32m    602\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[0;32m    603\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[0;32m    604\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[0;32m    605\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[0;32m    606\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[0;32m    607\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[0;32m    608\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[0;32m    609\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[0;32m    610\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[0;32m    611\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[0;32m    612\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[0;32m    613\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[0;32m    614\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[0;32m    615\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[0;32m    616\u001b[0m             },\n\u001b[0;32m    617\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[0;32m    618\u001b[0m         ),\n\u001b[0;32m    619\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[0;32m    620\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[0;32m    621\u001b[0m         ),\n\u001b[0;32m    622\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[0;32m    623\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    624\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[0;32m    625\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\xo103\\.conda\\envs\\geo_env\\lib\\site-packages\\openai\\_base_client.py:1055\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[0;32m   1042\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1043\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1050\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1051\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m   1052\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[0;32m   1053\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[0;32m   1054\u001b[0m     )\n\u001b[1;32m-> 1055\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[1;32mc:\\Users\\xo103\\.conda\\envs\\geo_env\\lib\\site-packages\\openai\\_base_client.py:834\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    825\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    826\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    827\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    832\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    833\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m--> 834\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[0;32m    835\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[0;32m    836\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[0;32m    837\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    838\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[0;32m    839\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[0;32m    840\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\xo103\\.conda\\envs\\geo_env\\lib\\site-packages\\openai\\_base_client.py:877\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    874\u001b[0m     \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    875\u001b[0m     \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[1;32m--> 877\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    879\u001b[0m     \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'Invalid content type. image_url is only supported by certain models.', 'type': 'invalid_request_error', 'param': 'messages.[0].content.[1].type', 'code': None}}"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import base64\n",
    "import mimetypes\n",
    "\n",
    "\n",
    "def api_call_gpt_with_retry(message_content:str, max_retries=3, delay=2):\n",
    "\n",
    "    client = AzureOpenAI(\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),  \n",
    "    api_version=\"2023-05-15\"\n",
    "    )\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    system_message = f'''\n",
    "    You are a excellent translator from Norwegian to British English (United Kingdom). \n",
    "    You are translating Norwegian consultant reports into english, so your language should be direct and professional.\n",
    "    DO NOT INCLUDE additional text, only translate the words provided in the prompt.\n",
    "    '''\n",
    "\n",
    "    prompt = f'''Translate the following text into British English: {message_content}\\n'''\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "    \n",
    "    def image_to_base64(image_path):\n",
    "        # Guess the MIME type of the image\n",
    "        mime_type, _ = mimetypes.guess_type(image_path)\n",
    "        \n",
    "        if not mime_type or not mime_type.startswith('image'):\n",
    "            raise ValueError(\"The file type is not recognized as an image\")\n",
    "        \n",
    "        # Read the image binary data\n",
    "        with open(image_path, 'rb') as image_file:\n",
    "            encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "        \n",
    "        # Format the result with the appropriate prefix\n",
    "        image_base64 = f\"data:{mime_type};base64,{encoded_string}\"\n",
    "    \n",
    "        return image_base64\n",
    "\n",
    "    base64_string = image_to_base64(\"image1.png\")\n",
    "\n",
    "    for attempt in range(max_retries): \n",
    "        response = client.chat.completions.create(\n",
    "                    model=\"gpt-4-vision-preview\",\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [\n",
    "                                {\"type\": \"text\", \"text\": message_content},\n",
    "                                {\n",
    "                                    \"type\": \"image_url\",\n",
    "                                    \"image_url\": {\n",
    "                                        \"url\": base64_string,\n",
    "                                        \"detail\": \"low\"\n",
    "                                    }\n",
    "                                },\n",
    "                            ],\n",
    "                        }\n",
    "                    ],\n",
    "                    max_tokens=300,\n",
    "                )\n",
    "\n",
    "        try:\n",
    "        \n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error {e}\")\n",
    "            print(response.text)\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                print(\"Max retries reached. Exiting.\")\n",
    "\n",
    "test = api_call_gpt_with_retry(\"Read the data from the picture below\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfortunately, I can't directly create a CSV file for you, but I can show you how the data should look in CSV format based on the image you provided. Here's how the data from the image would appear in a comma-separated values format:\n",
      "\n",
      "```\n",
      "name,rank,gender,year\n",
      "Jacob,1,boy,2010\n",
      "Isabella,1,girl,2010\n",
      "Ethan,2,boy,2010\n",
      "Sophia,2,girl,2010\n",
      "Michael,3,boy,2010\n",
      "```\n",
      "This text representation shows how the structured data from your image would be represented in a CSV file. If this were a complete dataset, you could continue with additional rows accordingly, each containing the name, rank, gender, and year information, up to the 2000 rows as mentioned at the bottom of the image.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import base64\n",
    "import mimetypes\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def image_to_base64(image_path):\n",
    "    # Guess the MIME type of the image\n",
    "    mime_type, _ = mimetypes.guess_type(image_path)\n",
    "    \n",
    "    if not mime_type or not mime_type.startswith('image'):\n",
    "        raise ValueError(\"The file type is not recognized as an image\")\n",
    "    \n",
    "    # Read the image binary data\n",
    "    with open(image_path, 'rb') as image_file:\n",
    "        encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "    # Format the result with the appropriate prefix\n",
    "    image_base64 = f\"data:{mime_type};base64,{encoded_string}\"\n",
    "    \n",
    "    return image_base64\n",
    "\n",
    "base64_string = image_to_base64(\"image1.png\")\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4-vision-preview\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Read the data from the picture and structure it as an csv\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": base64_string,\n",
    "                        \"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
